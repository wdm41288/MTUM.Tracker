%%writefile etf_tracker_auto.py
import pandas as pd
import os
import glob
import argparse
from datetime import datetime
import requests
import sys # For sys.exit

# --- Configuration ---
ETF_SYMBOL = "MTUM"
ETF_PRODUCT_ID = "251614"
ETF_SLUG = "ishares-msci-usa-momentum-factor-etf"
DOWNLOAD_URL = f"https://www.ishares.com/us/products/{ETF_PRODUCT_ID}/{ETF_SLUG}/1467271812596.ajax?fileType=csv&fileName={ETF_SYMBOL}_holdings&dataType=fund"

TICKER_COL = 'Ticker'
NAME_COL = 'Name'
WEIGHT_COL = 'Weight (%)'
ALT_WEIGHT_COLS = ['Weight', 'Weighting', 'Portfolio Weight', 'Market Weight']
DEFAULT_SAVE_DIR = "etf_data" # Relative path, good for GitHub Actions
MASTER_EXCEL_FILENAME_DEFAULT = f"{ETF_SYMBOL}_Master_Holdings.xlsx"

def download_mtum_holdings(save_directory):
    """Downloads MTUM holdings CSV file."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "Referer": f"https://www.ishares.com/us/products/{ETF_PRODUCT_ID}/{ETF_SLUG}/"
    }
    try:
        print(f"Attempting to download {ETF_SYMBOL} holdings from: {DOWNLOAD_URL}")
        response = requests.get(DOWNLOAD_URL, headers=headers, stream=True, timeout=60)
        response.raise_for_status()

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        raw_filename = f"{ETF_SYMBOL}_downloaded_content_{timestamp}.csv" # Tentative extension

        if not os.path.exists(save_directory):
            try:
                os.makedirs(save_directory)
                print(f"Created save directory: {save_directory}")
            except OSError as e:
                print(f"Error creating directory {save_directory}: {e}")
                return None

        filepath = os.path.join(save_directory, raw_filename)
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"Successfully downloaded content to: {filepath}")
        return filepath
    except requests.exceptions.Timeout:
        print(f"Error: Timeout occurred while trying to download from {DOWNLOAD_URL}")
        return None
    except requests.exceptions.ConnectionError:
        print(f"Error: Could not connect to {DOWNLOAD_URL}. Check internet connection or URL.")
        return None
    except requests.exceptions.HTTPError as e:
        print(f"Error: HTTP error {e.response.status_code} while downloading from {DOWNLOAD_URL}")
        error_preview = e.response.text[:500] if e.response.text else "No content"
        print(f"Response content preview: {error_preview}")
        if e.response.headers.get('Content-Type', '').startswith('text/html'):
            html_error_path = os.path.join(save_directory, f"{ETF_SYMBOL}_error_page_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html")
            try:
                with open(html_error_path, 'w', encoding='utf-8') as f_html:
                    f_html.write(e.response.text)
                print(f"HTML error page saved to: {html_error_path}")
            except Exception as e_save_html:
                 print(f"Could not save HTML error page: {e_save_html}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred during download: {e}")
        import traceback
        traceback.print_exc()
        return None

def load_and_process_holdings(downloaded_filepath):
    """Loads and processes the downloaded ETF holdings CSV file."""
    print(f"Processing downloaded file: {downloaded_filepath}")
    if not os.path.exists(downloaded_filepath):
        print(f"Error: File not found at {downloaded_filepath}")
        return None

    df = None
    skiprows = 0

    try:
        with open(downloaded_filepath, 'r', encoding='utf-8-sig') as f_preview:
            preview_lines = [f_preview.readline() for _ in range(20)] # Read more lines for better detection

        header_keywords_primary = [TICKER_COL.lower(), NAME_COL.lower()] # Key columns like Ticker, Name
        header_keywords_secondary = [WEIGHT_COL.lower()] # Weight column can vary more

        # Try to find a line that starts with "Ticker" or contains key headers
        # iShares files may have variable preamble lines.
        found_header_row = -1
        for i, line in enumerate(preview_lines):
            line_lower_stripped = line.lower().strip()
            if not line_lower_stripped: continue # Skip empty lines

            # Check if it looks like a header row
            if line_lower_stripped.startswith(TICKER_COL.lower()) or \
               (all(keyword in line_lower_stripped for keyword in header_keywords_primary) and \
                any(keyword in line_lower_stripped for keyword in header_keywords_secondary)):
                found_header_row = i
                break
        
        if found_header_row != -1:
            skiprows = found_header_row
            print(f"Detected header at line {skiprows + 1} (skiprows={skiprows}).")
        else:
            # Fallback for specific iShares CSV structure if keywords fail
            # For "fileName=MTUM_holdings" type, data often starts after line 9 (skiprows=9)
            # Other iShares files might skip 2 or 3 lines.
            # If the file starts with "Fund Holdings as of", "Holdings as of Date", it means we need to skip more.
            if any(line.strip().startswith("Fund Holdings as of") or line.strip().startswith("Holdings as of Date") for line in preview_lines[:5]):
                skiprows = 9 # A common value for such files
                print(f"Found 'Fund Holdings as of' type preamble. Setting skiprows to {skiprows} as a common default.")
            elif preview_lines[0].strip().startswith(TICKER_COL): # If Ticker is on the very first line
                skiprows = 0
                print("Header seems to be on the first line. Setting skiprows to 0.")
            else: # General fallback if specific patterns aren't met
                skiprows = 9 # Default for MTUM_holdings URL if specific detection fails
                print(f"Could not reliably detect header. Using default skiprows={skiprows} for {ETF_SYMBOL} specific URL.")

    except Exception as e_preview:
        print(f"Error during CSV preview/skiprows detection: {e_preview}. Will try common skips.")
        skiprows = 9 # Fallback to common skip if preview fails

    # Attempt to read CSV with detected or default skiprows
    read_successful = False
    for skip_attempt in [skiprows, 9, 2, 0]: # Try detected, then common fallbacks
        if read_successful: break
        try:
            print(f"Attempting to read CSV with skiprows={skip_attempt}")
            df = pd.read_csv(downloaded_filepath, encoding='utf-8-sig', skiprows=skip_attempt, on_bad_lines='warn')
            
            if df.empty or len(df.columns) < 2: # Check if df is substantially empty
                print(f"Read with skiprows={skip_attempt} resulted in an empty or near-empty DataFrame. Columns: {df.columns.tolist()}")
                df = None # Reset df to trigger next attempt or failure
                continue # Try next skip_attempt

            print(f"Successfully parsed CSV file with skiprows={skip_attempt}.")
            print(f"Columns found: {df.columns.tolist()}")
            read_successful = True
        except pd.errors.EmptyDataError:
            print(f"EmptyDataError: The CSV file at {downloaded_filepath} is empty or became empty after skipping {skip_attempt} rows.")
            df = None
        except Exception as e_csv:
            print(f"Failed to read CSV with skiprows={skip_attempt}: {e_csv}")
            df = None # Reset df

    if not read_successful or df is None or df.empty:
        print("All attempts to read CSV failed or resulted in an empty DataFrame.")
        try:
            with open(downloaded_filepath, 'r', encoding='utf-8-sig', errors='ignore') as f_err:
                print(f"--- Start of raw file content for {downloaded_filepath} ---")
                for _ in range(20): # Print more lines for debugging
                    line = f_err.readline()
                    if not line: break
                    print(line, end='')
                print(f"--- End of raw file content preview ---")
        except Exception as e_read_raw:
            print(f"Could not even read raw file for preview: {e_read_raw}")
        return None

    df.columns = [str(col).strip() for col in df.columns] # Strip spaces from column names

    actual_ticker_col, actual_name_col, actual_weight_col = None, None, None
    potential_cols = df.columns.tolist()

    # Find Ticker column
    for col_candidate in [TICKER_COL] + [c for c in potential_cols if 'ticker' in c.lower() or 'symbol' in c.lower()]:
        if col_candidate in potential_cols: actual_ticker_col = col_candidate; break
    if not actual_ticker_col and potential_cols: actual_ticker_col = potential_cols[0] # Default to first if not found

    # Find Name column
    for col_candidate in [NAME_COL] + [c for c in potential_cols if 'name' in c.lower() and 'company' not in c.lower() and 'identifier' not in c.lower()]:
        if col_candidate in potential_cols: actual_name_col = col_candidate; break
    if not actual_name_col and len(potential_cols) > 1: # Try to find a likely name col if not first
        for col_idx in range(1, min(3, len(potential_cols))): # Check 2nd or 3rd column
             if isinstance(df[potential_cols[col_idx]].iloc[0], str) and len(df[potential_cols[col_idx]].iloc[0]) > 3 : # Heuristic for name
                actual_name_col = potential_cols[col_idx]; break

    # Find Weight column
    for col_candidate in [WEIGHT_COL] + ALT_WEIGHT_COLS + [c for c in potential_cols if 'weight' in c.lower()]:
        if col_candidate in potential_cols: actual_weight_col = col_candidate; break

    if not actual_ticker_col: print(f"CRITICAL: Ticker column not found. Cols: {potential_cols}"); return None
    if not actual_weight_col: print(f"CRITICAL: Weight column not found. Cols: {potential_cols}"); return None
    if not actual_name_col: print(f"Warning: Name column not found. Will use Ticker. Cols: {potential_cols}")

    cols_to_select = {actual_ticker_col: 'Ticker', actual_weight_col: 'Weight'}
    if actual_name_col: cols_to_select[actual_name_col] = 'Name'
    
    try:
        holdings_df = df[list(cols_to_select.keys())].copy()
    except KeyError as e:
        print(f"KeyError during column selection: {e}. This means a selected column name wasn't actually in df.columns.")
        print(f"Selected map: {cols_to_select}, DataFrame columns: {df.columns.tolist()}")
        return None

    holdings_df.rename(columns=cols_to_select, inplace=True)
    if 'Name' not in holdings_df.columns: holdings_df['Name'] = holdings_df['Ticker']

    holdings_df['Ticker'] = holdings_df['Ticker'].astype(str).str.strip()
    invalid_tickers = ['nan', '-', '', 'ticker', 'fund holdings as of', 'average s&p market cap', 'net assets', 'total market value'] # Added more
    holdings_df = holdings_df[~holdings_df['Ticker'].str.lower().isin(invalid_tickers)]
    holdings_df = holdings_df[~holdings_df['Ticker'].str.match(r'\d{1,2}/\d{1,2}/\d{2,4}')] # M/D/YY or M/D/YYYY
    holdings_df = holdings_df[~holdings_df['Ticker'].str.lower().str.contains(r'cash and/or derivatives', na=False)]
    holdings_df = holdings_df[~holdings_df['Ticker'].str.lower().str.contains(r'total holdings', na=False)]


    if 'Weight' not in holdings_df.columns or holdings_df['Weight'].isnull().all():
        print(f"Error: 'Weight' column is missing or all NaN after selection. Check CSV and mappings.")
        return None

    if holdings_df['Weight'].dtype == 'object':
        holdings_df['Weight'] = holdings_df['Weight'].astype(str).str.replace(',', '', regex=False).str.rstrip('%').str.strip()
        holdings_df['Weight'] = holdings_df['Weight'].replace(['-', 'n/a', 'N/A', '', 'Market Value'], pd.NA, regex=False) # Added Market Value
    holdings_df['Weight'] = pd.to_numeric(holdings_df['Weight'], errors='coerce')

    original_rows = len(holdings_df)
    holdings_df.dropna(subset=['Ticker', 'Weight'], inplace=True)
    holdings_df = holdings_df[holdings_df['Weight'] > 0]
    if len(holdings_df) < original_rows:
        print(f"Dropped {original_rows - len(holdings_df)} rows due to invalid/missing Ticker/Weight or zero/negative weight.")

    if holdings_df.empty: print("Error: No valid holdings data after cleaning."); return None

    holdings_df = holdings_df.sort_values(by='Weight', ascending=False).reset_index(drop=True)
    holdings_df['Rank'] = holdings_df.index + 1
    final_cols = ['Rank', 'Ticker', 'Name', 'Weight']
    for col in final_cols:
        if col not in holdings_df.columns: holdings_df[col] = holdings_df.get('Ticker' if col == 'Name' else pd.NA)
    holdings_df = holdings_df[final_cols]

    print(f"\nProcessed {len(holdings_df)} holdings.")
    return holdings_df

def save_daily_csv(df, etf_symbol_for_csv, historical_data_dir):
    """Saves the current day's holdings to a CSV file in the historical data directory."""
    if df is None or df.empty:
        print("No data to save for daily CSV.")
        return
    if not os.path.exists(historical_data_dir):
        try:
            os.makedirs(historical_data_dir)
            print(f"Created historical data directory: {historical_data_dir}")
        except OSError as e:
            print(f"Error creating directory {historical_data_dir}: {e}. Cannot save daily CSV.")
            return

    date_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"{etf_symbol_for_csv}_holdings_{date_str}.csv"
    filepath = os.path.join(historical_data_dir, filename)
    try:
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        print(f"Successfully saved current holdings to daily CSV: {filepath}")
    except Exception as e:
        print(f"Error saving holdings to {filepath}: {e}")

def update_master_excel(df, master_excel_path):
    """Adds the current holdings as a new sheet to the master Excel file."""
    if df is None or df.empty:
        print("No data to update in master Excel file.")
        return

    date_str_sheet_name = datetime.now().strftime("%Y-%m-%d")
    print(f"Updating master Excel file: {master_excel_path} with sheet: {date_str_sheet_name}")

    master_excel_dir = os.path.dirname(master_excel_path)
    if master_excel_dir and not os.path.exists(master_excel_dir):
        try: os.makedirs(master_excel_dir); print(f"Created directory for master Excel: {master_excel_dir}")
        except OSError as e: print(f"Error creating directory for master Excel {master_excel_dir}: {e}"); return

    mode = 'a' if os.path.exists(master_excel_path) else 'w'
    if_sheet_exists_option = 'replace' if mode == 'a' else None # if file exists, replace sheet; else, None for new file
    engine_kwargs = {}
    if mode == 'w' and if_sheet_exists_option : # Should not happen with this logic but as safeguard for openpyxl
         pass # if_sheet_exists not valid for mode 'w' with openpyxl explicitly.

    try:
        with pd.ExcelWriter(master_excel_path, engine='openpyxl', mode=mode, if_sheet_exists=if_sheet_exists_option, engine_kwargs=engine_kwargs) as writer:
            df.to_excel(writer, sheet_name=date_str_sheet_name, index=False)
        print(f"Successfully updated master Excel file: {master_excel_path} with sheet '{date_str_sheet_name}'")
    except Exception as e:
        print(f"Error updating master Excel file {master_excel_path}: {e}")
        if "zipfile.BadZipFile" in str(e) or "File is not a zip file" in str(e):
             print("This might be due to an existing corrupted Excel file. Try deleting or checking its integrity.")
             print(f"Attempting to save as a new file if corruption detected and mode was 'a'...")
             if mode == 'a': # If append mode failed due to corruption, try writing new.
                 try:
                     new_master_path = master_excel_path.replace(".xlsx", f"_new_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx")
                     with pd.ExcelWriter(new_master_path, engine='openpyxl', mode='w') as writer_new:
                         df.to_excel(writer_new, sheet_name=date_str_sheet_name, index=False)
                     print(f"Saved to a new file due to corruption: {new_master_path}")
                 except Exception as e_new:
                     print(f"Could not save to a new file either: {e_new}")


def load_previous_holdings_from_csv(etf_symbol_for_csv, historical_data_dir):
    """Loads the most recent previous day's holdings from the historical CSVs."""
    if not os.path.exists(historical_data_dir):
        print(f"Historical data directory not found: {historical_data_dir}")
        return None

    list_of_files = glob.glob(os.path.join(historical_data_dir, f"{etf_symbol_for_csv}_holdings_*.csv"))
    if not list_of_files:
        print("No historical CSV files found.")
        return None

    today_date_str = datetime.now().strftime("%Y-%m-%d")
    today_filename_pattern = f"{etf_symbol_for_csv}_holdings_{today_date_str}.csv"
    previous_files = [f for f in list_of_files if os.path.basename(f) != today_filename_pattern]

    if not previous_files:
        print("No previous historical CSV files found (excluding today's).")
        return None
    try:
        latest_file = max(previous_files, key=os.path.getctime)
        print(f"Loading previous holdings from CSV: {latest_file}")
        return pd.read_csv(latest_file)
    except ValueError: print("No previous CSV files found to load."); return None
    except Exception as e: print(f"Error loading previous file {latest_file if 'latest_file' in locals() else 'unknown'}: {e}"); return None

def compare_holdings(current_df, previous_df, top_n=20):
    """Compares current holdings with previous holdings."""
    if previous_df is None or previous_df.empty: print("\nNo previous holdings to compare against."); return
    if current_df is None or current_df.empty: print("\nCurrent holdings are empty, cannot compare."); return
    print("\n--- Holdings Comparison ---")
    if 'Ticker' not in current_df.columns or 'Ticker' not in previous_df.columns:
        print("Error: 'Ticker' column missing for comparison."); return

    comparison_df = pd.merge(current_df, previous_df, on='Ticker', how='outer', suffixes=('_curr', '_prev'))
    for col_suffix in ['_curr', '_prev']:
        comparison_df[f'Rank{col_suffix}'].fillna(99999, inplace=True)
        comparison_df[f'Weight{col_suffix}'].fillna(0, inplace=True)
        if f'Name{col_suffix}' not in comparison_df.columns: comparison_df[f'Name{col_suffix}'] = ""
    comparison_df['Name_consolidated'] = comparison_df['Name_curr'].combine_first(comparison_df['Name_prev']).fillna(comparison_df['Ticker'])
    comparison_df['Rank_Change'] = comparison_df['Rank_prev'] - comparison_df['Rank_curr']
    comparison_df['Weight_Change'] = comparison_df['Weight_curr'] - comparison_df['Weight_prev']

    new_holdings = comparison_df[comparison_df['Weight_prev'] == 0][['Rank_curr', 'Ticker', 'Name_consolidated', 'Weight_curr']].rename(
        columns={'Rank_curr': 'Rank', 'Name_consolidated': 'Name', 'Weight_curr': 'Weight'}).sort_values(by='Rank')
    dropped_holdings = comparison_df[comparison_df['Weight_curr'] == 0][['Rank_prev', 'Ticker', 'Name_consolidated', 'Weight_prev']].rename(
        columns={'Rank_prev': 'Old Rank', 'Name_consolidated': 'Name', 'Weight_prev': 'Old Weight'}).sort_values(by='Old Rank')
    changed_holdings = comparison_df[(comparison_df['Weight_curr'] > 0) & (comparison_df['Weight_prev'] > 0)].rename(
        columns={'Name_consolidated': 'Name'}).sort_values(by='Rank_curr')

    print(f"\nTop {top_n} Current Holdings:\n{current_df[['Rank', 'Ticker', 'Name', 'Weight']].head(top_n).to_string(index=False)}")
    if not new_holdings.empty: print(f"\nNew Holdings (Top {top_n} of new):\n{new_holdings.head(top_n).to_string(index=False)}")
    else: print("\nNo new holdings found.")
    if not dropped_holdings.empty: print(f"\nDropped Holdings (Top {top_n} of prev rank):\n{dropped_holdings[dropped_holdings['Old Rank'] <= top_n].head(top_n).to_string(index=False)}")
    else: print("\nNo holdings dropped.")

    if not changed_holdings.empty:
        sig_rank_chg = changed_holdings[(abs(changed_holdings['Rank_Change']) > 5) | \
                                        ((changed_holdings['Rank_curr'] <= top_n) & (changed_holdings['Rank_prev'] > top_n)) | \
                                        ((changed_holdings['Rank_curr'] > top_n) & (changed_holdings['Rank_prev'] <= top_n))]\
                                        .sort_values(by=lambda x: abs(x['Rank_Change']), ascending=False)
        if not sig_rank_chg.empty: print(f"\nSignificant Rank Changes (Top {top_n} by magnitude):\n{sig_rank_chg[['Rank_curr', 'Ticker', 'Name', 'Rank_prev', 'Rank_Change', 'Weight_curr', 'Weight_prev', 'Weight_Change']].head(top_n).to_string(index=False)}")
        else: print("\nNo significant rank changes meeting criteria.")
        sig_wght_chg = changed_holdings[abs(changed_holdings['Weight_Change']) > 0.1].copy()
        sig_wght_chg = sig_wght_chg.reindex(sig_wght_chg['Weight_Change'].abs().sort_values(ascending=False).index)
        if not sig_wght_chg.empty: print(f"\nSignificant Weight Changes (> +/- 0.1%, Top {top_n} by magnitude):\n{sig_wght_chg[['Rank_curr', 'Ticker', 'Name', 'Weight_curr', 'Weight_prev', 'Weight_Change']].head(top_n).to_string(index=False)}")
        else: print("\nNo significant weight changes (> +/- 0.1%).")
    else: print("\nNo continuously held stocks to compare changes for.")

def main():
    parser = argparse.ArgumentParser(description=f"Track {ETF_SYMBOL} ETF holdings, save to CSV and a master Excel file.")
    parser.add_argument("--save_dir", default=DEFAULT_SAVE_DIR, help=f"Main directory for outputs. Default: './{DEFAULT_SAVE_DIR}'")
    parser.add_argument("--etf_symbol_for_filename", default=ETF_SYMBOL, help=f"ETF symbol for output file names. Default: {ETF_SYMBOL}")
    parser.add_argument("--top_n", type=int, default=20, help="Top N holdings for summaries. Default: 20")
    parser.add_argument("--master_excel_name", default=MASTER_EXCEL_FILENAME_DEFAULT, help=f"Master Excel file name. Default: '{MASTER_EXCEL_FILENAME_DEFAULT}' in save_dir.")

    args = parser.parse_args() if not any(arg for arg in sys.argv if 'ipykernel_launcher' in arg or 'colab_kernel_launcher' in arg) else parser.parse_args([])
    print(f"--- Starting ETF Holdings Tracker for: {args.etf_symbol_for_filename} ---")

    download_target_dir = args.save_dir
    historical_csv_dir = os.path.join(args.save_dir, "historical_csvs")
    master_excel_filepath = os.path.join(args.save_dir, args.master_excel_name)

    print(f"Raw downloaded files will be in: {download_target_dir}")
    print(f"Historical daily CSVs will be in: {historical_csv_dir}")
    print(f"Master Excel file: {master_excel_filepath}")

    downloaded_content_path = download_mtum_holdings(download_target_dir)
    if not downloaded_content_path: print("Failed to download ETF holdings. Exiting."); sys.exit(1)

    current_holdings_df = load_and_process_holdings(downloaded_content_path)
    if current_holdings_df is None or current_holdings_df.empty:
        print("Could not process downloaded holdings. Exiting."); sys.exit(1)

    print(f"\n--- Top {args.top_n} Current Holdings ({args.etf_symbol_for_filename}) {datetime.now().strftime('%Y-%m-%d')} ---\n{current_holdings_df.head(args.top_n).to_string(index=False)}")
    save_daily_csv(current_holdings_df, args.etf_symbol_for_filename, historical_csv_dir)
    update_master_excel(current_holdings_df, master_excel_filepath)
    previous_holdings_df = load_previous_holdings_from_csv(args.etf_symbol_for_filename, historical_csv_dir)
    compare_holdings(current_holdings_df, previous_hold_df, top_n=args.top_n) if previous_holdings_df is not None else print("\nNo previous holdings CSV for comparison.")
    print("\n--- ETF Tracker Finished ---")

if __name__ == "__main__":
    main()
if __name__ == "__main__":
    main()
